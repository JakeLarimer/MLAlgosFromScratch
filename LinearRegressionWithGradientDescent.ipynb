{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78769aaf",
   "metadata": {},
   "source": [
    "# Gradient Descent Applied To Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45a726",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/\n",
    "https://www.youtube.com/watch?v=YS_EztqZCD8\n",
    "https://introml.mit.edu/notes/gradient_descent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc448e",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Gradient descent is a method used to train a variety of machine learning algorithms. In this notebook, we will use a few common gradient descent varients to perform linear regression, and explore how gradient descent works. Although linear regression has a closed form solution (I go through that here [MultipleLinear+PolynomialRegression](https://github.com/JakeLarimer/MLAlgosFromScratch/blob/main/MultipleLinearRegression%2BPolynomialRegression.ipynb)), other algorithms often do not, requiring gradient descent to learn. Given the simplicity of linear regression compared to other algorithms, it acts as a good vessel to demonstrate the application of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c8c4",
   "metadata": {},
   "source": [
    "The ovarching goal of gradient descent is to identify model parameters that lead to a local minima of a \"cost\" or \"loss\" function. These functions essentially defines the error between the predicted values from a model, and the actual values. Loss and cost functions are similar, but have an important distinction. Loss refers to the sum of the function over all predicted and true values, while cost is the average error per value. Different algorithms use different loss functions. For example with linear regression, we use the $\\textit{sum of squared errors}$ or $\\textit{SSE}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bc0b5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is a true observation and $\\hat{y}_i$ is the corresponing predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe96c1",
   "metadata": {},
   "source": [
    "Loss functions are generally denoted as $J(\\Theta)$ where $\\Theta$ is a vector containing the model parameters. Thus, our goal is to find $\\Theta^*$ such that $\\Theta^*=\\arg \\min_{\\Theta}J(\\Theta)$. With gradient descent, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798f21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d43f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea292c1-775f-4276-be9f-6c71524042d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
