{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9c6eef",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de48c71",
   "metadata": {},
   "source": [
    "Sources:  \n",
    "  \n",
    "https://www.stat.purdue.edu/~zhanghao/MAS/handout/Likelihood.pdf  \n",
    "https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html  \n",
    "https://math.unm.edu/~schrader/biostat/bio1/notes/lecture15.pdf  \n",
    "https://online.stat.psu.edu/stat501/lesson/13/13.2  \n",
    "https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/  \n",
    "https://www.geeksforgeeks.org/data-science/likelihood-function/  \n",
    "https://medium.com/@robdelacruz/logistic-regression-from-scratch-7db690fb380b  \n",
    "https://medium.com/@nicolasanti_43152/ml-loss-and-likelihood-e0a5ff4ae594  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e23deb",
   "metadata": {},
   "source": [
    "For binary data $y\\in\\{0, 1\\}$ with $\\pi=p(y=1)$ and $1-\\pi=p(y=0)$, the probability mass function (pmf) is:\n",
    "\n",
    "$$\n",
    "p(y, \\pi) = \\pi^y(1-\\pi)^{1-y}\n",
    "$$\n",
    "\n",
    "This is commonly reffered to as a \"Bernoulli Distribution\".\n",
    "\n",
    "Given m predictors and n samples, we can arrange our predictors into an (m+1) x n matrix $X$. For cleanliness, we can make the first column vecor of $X$, $x_0$, filled with $1$s to be used with the intercept parameter.\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\cdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nm}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each row vector, $x_i$, represents the feature values corresponding to the i'th observation, and each column vector, $x_j$, contains the predictor values for a single feature across all n samples.\n",
    "\n",
    "Logistic regression relies on the assumption that we can relate $p(y)$ to $X$ using the below:\n",
    "\n",
    "$$\n",
    "\\log(\\frac{p_i}{1-p_i})=\\sum_{j=1}^m x_ij \\theta_j=x_i^T\\Theta \\quad \\xrightarrow{} \\quad p(y_i=1 | x_i, \\Theta) = \\frac{e^{x_i^T\\Theta}}{1 + e^{x_i^T\\Theta}}\n",
    "$$\n",
    "\n",
    "where $\\Theta$ is the paremter vector, and thus any $\\theta_j$ is a given scalar parameter corresponding to a predictor vector $x_j$.\n",
    "\n",
    "\n",
    "For intuition, note that\n",
    "\n",
    "$$\n",
    "\\lim_{p\\xrightarrow{}1}\\log(\\frac{p}{1-p})=+\\infty\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\lim_{p\\xrightarrow{}0}\\log(\\frac{p}{1-p})=-\\infty\n",
    "$$\n",
    "\n",
    "so simply put, when $x_i^T\\Theta$ is large, it indicates a high probability that $y=1$, and when very negative, a high probability that $y=0$.\n",
    "\n",
    "######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d9702",
   "metadata": {},
   "source": [
    "Now, let's introduce something called the \"Likelihood Function\":\n",
    "It is defined as the likelihood of the parameters given the data.\n",
    "\n",
    "So, the likelihood for a binary logistic classification given n samples follows the below:\n",
    "\n",
    "$$\n",
    "L(\\Theta | \\mathbf{Y,X})=\\prod_{i=0}^n p(y_i| \\pi_i) = \\prod_{i=0}^n \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\prod_{i=0}^n \\frac{e^{x_i^T\\Theta}}{1 + e^{x_i^T\\Theta}}\n",
    "$$\n",
    "\n",
    "\n",
    "And thus the Log-Likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(\\Theta | \\mathbf{Y,X}) = \\sum_{i=0}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]\n",
    "$$\n",
    "\n",
    "When we find a maximum of the Log-Likelihood function, we have determined the values of $\\Theta$ that maximizes the probability of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abaef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
